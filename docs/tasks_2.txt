*) в папке docs/howto вести конспекты по настройке отдельных частей системы (как развернуть HDFS, k8s, docker, fastapi, CI/CD, Redis, Kafka, Airflow и прочее)
*) добавить папку black_jack_and_whores

Глобально:
*) сделать "энтерпрайзный" (распределенный с Big Data технологиями) топовый по качеству сервис рекомендаций
*) научиться делать расчет нагрузки, скорости, надежности для распределенной системы в целом, отдельных нод
*) прикрутить анализ и прогнозирование временных рядов по товарам
?) прикрутить моделирование новых данных (GAN над табличными данными)
?) сделать фронт с поиском похожих товаров (задача матчинга, поисковых подсказок, ранжирования, DSSM, FastText, LLM)

*) запаковать пайплайн обучения бейзлайна в сервис
*) запаковать обученную модель в сервис
*) сделать клиент (для тестов использовать уже имеющиеся id + их дубликаты для теста кэша, а для нормальной работы либо submission_sample ids, либо id от тестовых данных)
*) выкатить и опробовать сервисы на виртуальные ноды без кубера (Docker, Docker Compose)
*) выкатить через кубер
*) построить пайплайн обучения и выкатки в прод (kubernates, kubeflow, celery, airflow)
*) настроить hdfs
*) запихать изображения и обученные модели в hdfs
*) перенести с pandas на Spark
*) настроить мониторинг работы сервисов (нагрузка, задержки, состояние, занятое место, узлы)
*) настроить дашборды (метрики прогнозов, объем данных, объем новых данных, инфа о текущей модели, выручка)
?) собирать инфу о том, что перекомендовала модель, и что было куплено реально (понятно, что это не живые данные, но ради интереса можно)?
*) написать тесты + настроить CI/CD (jenkins)
*) развернуть хотя бы один сервис в облаке (yandex cloud)
*) прикрутить MLFlow
*) следующаая итерация с реранкером 
*) подсчет метрик ранжирования
?) дашборд-рубрикатор (тематизировать товары, выбор товара из интересующей категории, отрисовка графика продаж, прогноза на следующий период, показатели выручки (абс, отн реальный/ожидаемый рост))
---------------------------------------------

API Gateway:
*) GET рекомендация для id юзера
*) GET ретрейн модели

сервис обучения модели:
*) мониторит топик retrain_model в kafka
*) сообщение получено
*) подтягивает транзакции с бэка
*) ретрейн
*) сериализация модели в s3 (HDFS/dvc/MLFLow/MinIO)
*) отправка сообщения в топик model_updated
*) отправка сообщений на обновление рекомендаций в топик refresh_recommendations (взять все id из модели, для каждого составить сообщение на обновление рекомендаций)
?) подкрутить MLFlow?

сервис обновления рекомендаций:
*) мониторит топики requests_for_predictions, model_updated, refresh_recommendations
*) сообщение получено
*) инференс
*) отправка сообщения с результатом предикта в топик recommendation_updates
?) если обновлена модель, то пробовать асинхронно подгружать и заменять старую на новую или как-то перекладывать работу на Kuber и перезапускать конкретный сервис для подгрузки новой модели (запускать джобу на последовательный перезапуск каждого сервиса, чтобы остальные продолжали работать)? 

сервис предоставления рекомендаций:
*) мониторит топик recommendation_updates (асинхронно обновляет записи в Redis)
*) прием get запроса (тело - id юзера)
*) смотрит в Redis:
	*) если id есть, то отдает индивидуальную 	рекомендацию
	*) если id нет, то отдает stub (например, самые 	популярные 	товары)

сервис логирования транзакций:
*) мониторит топик new_transactions
*) сообщение получено
*) обновляет СУБД на бэке (Postgre)
?) каждые N (или % новых, или временной промежуток M) транзакций отправляет сообщение в топик retrain_model?

сервис имитации совершения транзакций:
*) загружает в RAM батч с N последними транзакциями (делить СУБД перед выкаткой сервисов, скрипт миграции части данных в СУБД на бэке?)
*) шлет асинхронно каждую транзакцию сообщением в топик  new_transactions с периодом M (использовать реальные промежутки?)

клиент рекомендательного сервиса:
*) асинхронно отправляет get запросы на рекомендацию для текущего user id (брать из sample_submission)
*) получает json response
*) хранит все в RAM
*) если id исчерпаны, то дожидается всех ответов, проверяет все ли получены (если нет, то отправляет повторно) и сохраняет все в csv
?) отправка post запроса на обновление модели?